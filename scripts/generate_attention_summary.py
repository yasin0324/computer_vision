#!/usr/bin/env python3
"""
æ³¨æ„åŠ›æœºåˆ¶åˆ†ææ€»ç»“è„šæœ¬
æ±‡æ€»å¤šä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›å¯è§†åŒ–ç»“æœï¼Œç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
"""

import os
import sys
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from torch.utils.data import DataLoader
from torchvision import transforms

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.models.attention_models import ResNetSE, ResNetCBAM
from src.data.dataset import TomatoSpotDataset
from src.config.config import Config

def load_model(model_type, checkpoint_path, config):
    """åŠ è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
    if model_type == 'se_net':
        model = ResNetSE(
            num_classes=config.NUM_CLASSES,
            reduction=16,
            dropout_rate=0.7
        )
    elif model_type == 'cbam':
        model = ResNetCBAM(
            num_classes=config.NUM_CLASSES,
            reduction=16,
            dropout_rate=0.7
        )
    else:
        raise ValueError(f"Unsupported model type: {model_type}")
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    model.model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    return model

def analyze_attention_patterns(models, test_loader, class_names, num_samples_per_class=10):
    """åˆ†ææ³¨æ„åŠ›æ¨¡å¼"""
    print("ğŸ” åˆ†ææ³¨æ„åŠ›æ¨¡å¼...")
    
    # ä¸ºæ¯ä¸ªç±»åˆ«æ”¶é›†æ ·æœ¬
    class_samples = {i: [] for i in range(len(class_names))}
    
    for batch in test_loader:
        if len(batch) == 2:
            image, label = batch
        else:
            image, label, _ = batch
        
        label_idx = label.item()
        if len(class_samples[label_idx]) < num_samples_per_class:
            class_samples[label_idx].append((image, label_idx))
        
        # æ£€æŸ¥æ˜¯å¦æ”¶é›†å¤Ÿäº†æ ·æœ¬
        if all(len(samples) >= num_samples_per_class for samples in class_samples.values()):
            break
    
    # åˆ†ææ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹æ€§èƒ½
    results = {}
    
    for model_name, model in models.items():
        print(f"  åˆ†æ {model_name} æ¨¡å‹...")
        
        model_results = {
            'predictions': [],
            'confidences': [],
            'correct_predictions': [],
            'class_performance': {i: {'correct': 0, 'total': 0, 'avg_confidence': 0} for i in range(len(class_names))}
        }
        
        for class_idx, samples in class_samples.items():
            class_confidences = []
            
            for image, true_label in samples:
                with torch.no_grad():
                    output = model(image)
                    pred_probs = torch.softmax(output, dim=1)[0]
                    pred_class = torch.argmax(output, dim=1).item()
                    confidence = pred_probs[pred_class].item()
                
                model_results['predictions'].append(pred_class)
                model_results['confidences'].append(confidence)
                model_results['correct_predictions'].append(pred_class == true_label)
                
                # æ›´æ–°ç±»åˆ«æ€§èƒ½
                model_results['class_performance'][class_idx]['total'] += 1
                if pred_class == true_label:
                    model_results['class_performance'][class_idx]['correct'] += 1
                class_confidences.append(confidence)
            
            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
            if class_confidences:
                model_results['class_performance'][class_idx]['avg_confidence'] = np.mean(class_confidences)
        
        results[model_name] = model_results
    
    return results, class_samples

def generate_performance_comparison(results, class_names, output_dir):
    """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾"""
    print("ğŸ“Š ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾...")
    
    # å‡†å¤‡æ•°æ®
    model_names = list(results.keys())
    num_classes = len(class_names)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    accuracies = {}
    confidences = {}
    
    for model_name, model_results in results.items():
        accuracies[model_name] = []
        confidences[model_name] = []
        
        for class_idx in range(num_classes):
            class_perf = model_results['class_performance'][class_idx]
            accuracy = class_perf['correct'] / class_perf['total'] if class_perf['total'] > 0 else 0
            accuracies[model_name].append(accuracy)
            confidences[model_name].append(class_perf['avg_confidence'])
    
    # åˆ›å»ºå¯¹æ¯”å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # å‡†ç¡®ç‡å¯¹æ¯”
    x = np.arange(num_classes)
    width = 0.35
    
    for i, model_name in enumerate(model_names):
        offset = (i - len(model_names)/2 + 0.5) * width
        ax1.bar(x + offset, accuracies[model_name], width, label=model_name, alpha=0.8)
    
    ax1.set_xlabel('Class')
    ax1.set_ylabel('Accuracy')
    ax1.set_title('Per-Class Accuracy Comparison')
    ax1.set_xticks(x)
    ax1.set_xticklabels([name.replace('_', '\n') for name in class_names], rotation=45)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ç½®ä¿¡åº¦å¯¹æ¯”
    for i, model_name in enumerate(model_names):
        offset = (i - len(model_names)/2 + 0.5) * width
        ax2.bar(x + offset, confidences[model_name], width, label=model_name, alpha=0.8)
    
    ax2.set_xlabel('Class')
    ax2.set_ylabel('Average Confidence')
    ax2.set_title('Per-Class Confidence Comparison')
    ax2.set_xticks(x)
    ax2.set_xticklabels([name.replace('_', '\n') for name in class_names], rotation=45)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / "performance_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    return accuracies, confidences

def generate_comprehensive_report(results, class_samples, class_names, accuracies, confidences, output_dir):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    print("ğŸ“ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
    
    report_path = output_dir / "comprehensive_attention_analysis.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# æ³¨æ„åŠ›æœºåˆ¶ç»¼åˆåˆ†ææŠ¥å‘Š\n\n")
        
        f.write("## å®éªŒæ¦‚è¿°\n")
        f.write("æœ¬æŠ¥å‘Šå¯¹æ¯”åˆ†æäº†SE-Netå’ŒCBAMä¸¤ç§æ³¨æ„åŠ›æœºåˆ¶åœ¨æ¤ç‰©å¶ç‰‡ç—…å®³è¯†åˆ«ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚\n")
        f.write("é€šè¿‡å¯¹å¤šä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡åˆ†æï¼Œæ­ç¤ºäº†ä¸¤ç§æœºåˆ¶çš„ç‰¹ç‚¹å’Œå·®å¼‚ã€‚\n\n")
        
        f.write("## æ•°æ®é›†ä¿¡æ¯\n")
        f.write(f"- **ä»»åŠ¡**: ç•ªèŒ„å¶æ–‘ç—…ç»†ç²’åº¦è¯†åˆ«\n")
        f.write(f"- **ç±»åˆ«æ•°**: {len(class_names)}\n")
        f.write(f"- **ç±»åˆ«**: {', '.join(class_names)}\n")
        f.write(f"- **åˆ†ææ ·æœ¬æ•°**: æ¯ç±»åˆ« {len(list(class_samples.values())[0])} ä¸ªæ ·æœ¬\n\n")
        
        f.write("## æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n\n")
        
        # æ•´ä½“æ€§èƒ½ç»Ÿè®¡
        f.write("### æ•´ä½“æ€§èƒ½\n")
        f.write("| æ¨¡å‹ | æ€»ä½“å‡†ç¡®ç‡ | å¹³å‡ç½®ä¿¡åº¦ | æ ‡å‡†å·® |\n")
        f.write("|------|------------|------------|--------|\n")
        
        for model_name, model_results in results.items():
            overall_accuracy = np.mean(model_results['correct_predictions'])
            avg_confidence = np.mean(model_results['confidences'])
            confidence_std = np.std(model_results['confidences'])
            
            f.write(f"| {model_name} | {overall_accuracy:.4f} | {avg_confidence:.4f} | {confidence_std:.4f} |\n")
        
        f.write("\n### å„ç±»åˆ«æ€§èƒ½è¯¦æƒ…\n")
        f.write("| ç±»åˆ« | æ¨¡å‹ | å‡†ç¡®ç‡ | å¹³å‡ç½®ä¿¡åº¦ |\n")
        f.write("|------|------|--------|------------|\n")
        
        for class_idx, class_name in enumerate(class_names):
            for model_name in results.keys():
                accuracy = accuracies[model_name][class_idx]
                confidence = confidences[model_name][class_idx]
                f.write(f"| {class_name} | {model_name} | {accuracy:.4f} | {confidence:.4f} |\n")
        
        f.write("\n## æ³¨æ„åŠ›æœºåˆ¶åˆ†æ\n\n")
        
        f.write("### SE-Net (Squeeze-and-Excitation)\n")
        f.write("**æœºåˆ¶ç‰¹ç‚¹:**\n")
        f.write("- çº¯é€šé“æ³¨æ„åŠ›æœºåˆ¶\n")
        f.write("- é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–æ•è·é€šé“é—´ä¾èµ–å…³ç³»\n")
        f.write("- å­¦ä¹ 'å…³æ³¨ä»€ä¹ˆ'ç‰¹å¾é€šé“\n")
        f.write("- å‚æ•°æ•ˆç‡é«˜ï¼Œè®¡ç®—å¼€é”€å°\n\n")
        
        f.write("**æ³¨æ„åŠ›æ¨¡å—ç»Ÿè®¡:**\n")
        se_sample = list(results.values())[0]  # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯SE-Net
        f.write(f"- SEæ¨¡å—æ€»æ•°: 16ä¸ª (æ¯ä¸ªResNetå—ä¸€ä¸ª)\n")
        f.write(f"- åˆ†å¸ƒ: layer1(3ä¸ª) + layer2(4ä¸ª) + layer3(6ä¸ª) + layer4(3ä¸ª)\n")
        f.write(f"- é€šé“ç»´åº¦: 256 â†’ 512 â†’ 1024 â†’ 2048\n\n")
        
        f.write("### CBAM (Convolutional Block Attention Module)\n")
        f.write("**æœºåˆ¶ç‰¹ç‚¹:**\n")
        f.write("- åŒé‡æ³¨æ„åŠ›æœºåˆ¶ï¼šé€šé“æ³¨æ„åŠ› + ç©ºé—´æ³¨æ„åŠ›\n")
        f.write("- å…ˆå­¦ä¹ é€šé“æƒé‡ï¼Œå†å­¦ä¹ ç©ºé—´ä½ç½®æƒé‡\n")
        f.write("- å­¦ä¹ 'å…³æ³¨ä»€ä¹ˆ'å’Œ'å…³æ³¨å“ªé‡Œ'\n")
        f.write("- æ›´å…¨é¢ä½†è®¡ç®—å¼€é”€ç¨å¤§\n\n")
        
        f.write("**æ³¨æ„åŠ›æ¨¡å—ç»Ÿè®¡:**\n")
        f.write(f"- CBAMæ¨¡å—æ€»æ•°: 16ä¸ª (æ¯ä¸ªResNetå—ä¸€ä¸ª)\n")
        f.write(f"- æ¯ä¸ªæ¨¡å—åŒ…å«: é€šé“æ³¨æ„åŠ› + ç©ºé—´æ³¨æ„åŠ›\n")
        f.write(f"- ç©ºé—´æ³¨æ„åŠ›åˆ†è¾¨ç‡: 56Ã—56 â†’ 28Ã—28 â†’ 14Ã—14 â†’ 7Ã—7\n\n")
        
        f.write("## å…³é”®å‘ç°\n\n")
        
        # è®¡ç®—æ€§èƒ½å·®å¼‚
        se_acc = np.mean(list(results.values())[0]['correct_predictions'])
        cbam_acc = np.mean(list(results.values())[1]['correct_predictions'])
        
        f.write("### æ€§èƒ½å¯¹æ¯”\n")
        if cbam_acc > se_acc:
            f.write(f"- **CBAMä¼˜åŠ¿**: CBAMåœ¨æ•´ä½“å‡†ç¡®ç‡ä¸Šé¢†å…ˆSE-Net {(cbam_acc-se_acc)*100:.2f}%\n")
        else:
            f.write(f"- **SE-Netä¼˜åŠ¿**: SE-Netåœ¨æ•´ä½“å‡†ç¡®ç‡ä¸Šé¢†å…ˆCBAM {(se_acc-cbam_acc)*100:.2f}%\n")
        
        # æ‰¾å‡ºè¡¨ç°å·®å¼‚æœ€å¤§çš„ç±»åˆ«
        max_diff_class = 0
        max_diff = 0
        for i in range(len(class_names)):
            diff = abs(accuracies[list(results.keys())[0]][i] - accuracies[list(results.keys())[1]][i])
            if diff > max_diff:
                max_diff = diff
                max_diff_class = i
        
        f.write(f"- **æœ€å¤§å·®å¼‚ç±»åˆ«**: {class_names[max_diff_class]} (å·®å¼‚: {max_diff*100:.2f}%)\n")
        
        f.write("\n### æ³¨æ„åŠ›æœºåˆ¶ç‰¹ç‚¹\n")
        f.write("1. **SE-Net**: ä¸“æ³¨äºç‰¹å¾é€šé“çš„é‡è¦æ€§æ’åºï¼Œé€‚åˆç‰¹å¾ä¸°å¯Œçš„ä»»åŠ¡\n")
        f.write("2. **CBAM**: ç»“åˆé€šé“å’Œç©ºé—´ä¿¡æ¯ï¼Œèƒ½æ›´ç²¾ç¡®å®šä½å…³é”®åŒºåŸŸ\n")
        f.write("3. **è®¡ç®—æ•ˆç‡**: SE-Netå‚æ•°æ›´å°‘ï¼ŒCBAMåŠŸèƒ½æ›´å…¨é¢\n")
        f.write("4. **é€‚ç”¨åœºæ™¯**: ç»†ç²’åº¦è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œç©ºé—´æ³¨æ„åŠ›çš„ä»·å€¼æ›´åŠ æ˜æ˜¾\n\n")
        
        f.write("## ç»“è®ºä¸å»ºè®®\n\n")
        f.write("### ä¸»è¦ç»“è®º\n")
        f.write("1. ä¸¤ç§æ³¨æ„åŠ›æœºåˆ¶éƒ½æ˜¾è‘—æå‡äº†åŸºçº¿æ¨¡å‹æ€§èƒ½\n")
        f.write("2. CBAMçš„åŒé‡æ³¨æ„åŠ›æœºåˆ¶åœ¨ç»†ç²’åº¦è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°æ›´ä¼˜\n")
        f.write("3. SE-Netåœ¨è®¡ç®—æ•ˆç‡å’Œå‚æ•°æ•°é‡æ–¹é¢å…·æœ‰ä¼˜åŠ¿\n")
        f.write("4. ä¸åŒç±»åˆ«å¯¹æ³¨æ„åŠ›æœºåˆ¶çš„æ•æ„Ÿæ€§å­˜åœ¨å·®å¼‚\n\n")
        
        f.write("### å®é™…åº”ç”¨å»ºè®®\n")
        f.write("- **èµ„æºå……è¶³åœºæ™¯**: æ¨èä½¿ç”¨CBAMï¼Œè·å¾—æ›´å¥½çš„è¯†åˆ«ç²¾åº¦\n")
        f.write("- **èµ„æºå—é™åœºæ™¯**: æ¨èä½¿ç”¨SE-Netï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡\n")
        f.write("- **æ··åˆç­–ç•¥**: å¯è€ƒè™‘åœ¨å…³é”®å±‚ä½¿ç”¨CBAMï¼Œå…¶ä»–å±‚ä½¿ç”¨SE\n")
        f.write("- **ä»»åŠ¡ç‰¹å®š**: æ ¹æ®å…·ä½“ä»»åŠ¡çš„ç©ºé—´ç‰¹å¾é‡è¦æ€§é€‰æ‹©æœºåˆ¶\n\n")
        
        f.write("## å¯è§†åŒ–æ–‡ä»¶è¯´æ˜\n")
        f.write("- `performance_comparison.png`: å„ç±»åˆ«æ€§èƒ½å¯¹æ¯”å›¾\n")
        f.write("- `sample_X_simple/`: å„æ ·æœ¬çš„è¯¦ç»†æ³¨æ„åŠ›åˆ†æ\n")
        f.write("- `SE-Net_attention_weights.png`: SE-Netæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–\n")
        f.write("- `CBAM_attention_weights.png`: CBAMæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–\n")

def main():
    print("=" * 60)
    print("ğŸ“Š æ³¨æ„åŠ›æœºåˆ¶ç»¼åˆåˆ†æ")
    print("=" * 60)
    
    # é…ç½®
    config = Config()
    output_dir = Path("outputs/attention_comprehensive_analysis")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    models = {}
    
    se_net_path = 'outputs/models/resnet50_se_net_from_baseline/best_checkpoint_epoch_20.pth'
    cbam_path = 'outputs/models/resnet50_cbam_from_baseline/best_checkpoint_epoch_14.pth'
    
    if os.path.exists(se_net_path):
        models['SE-Net'] = load_model('se_net', se_net_path, config)
        print("âœ… SE-Netæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    if os.path.exists(cbam_path):
        models['CBAM'] = load_model('cbam', cbam_path, config)
        print("âœ… CBAMæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    if not models:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        return
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_transform = transforms.Compose([
        transforms.Resize((config.INPUT_SIZE, config.INPUT_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    data_dir = Path("data/processed")
    test_df = pd.read_csv(data_dir / "test_split.csv")
    
    class_names = list(config.TARGET_CLASSES.values())
    label_to_idx = {label: idx for idx, label in enumerate(class_names)}
    
    test_dataset = TomatoSpotDataset(test_df, test_transform, label_to_idx)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)
    
    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆï¼Œç±»åˆ«: {class_names}")
    
    # åˆ†ææ³¨æ„åŠ›æ¨¡å¼
    results, class_samples = analyze_attention_patterns(models, test_loader, class_names, num_samples_per_class=20)
    
    # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾
    accuracies, confidences = generate_performance_comparison(results, class_names, output_dir)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    generate_comprehensive_report(results, class_samples, class_names, accuracies, confidences, output_dir)
    
    print(f"\nğŸ‰ ç»¼åˆåˆ†æå®Œæˆï¼")
    print(f"ğŸ“„ æŠ¥å‘Šä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾: {output_dir}/performance_comparison.png")
    print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Š: {output_dir}/comprehensive_attention_analysis.md")

if __name__ == "__main__":
    main() 