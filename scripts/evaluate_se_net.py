#!/usr/bin/env python3
"""
SE-Netæ¨¡å‹è¯„ä¼°è„šæœ¬
"""

import sys
import os
import argparse
from pathlib import Path
import json
from datetime import datetime

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import pandas as pd
from torchvision import transforms
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.models.attention_models import create_resnet_with_attention
from src.data.dataset import TomatoSpotDataset
from src.config.config import config


def load_se_net_model(checkpoint_path, device):
    """åŠ è½½SE-Netæ¨¡å‹"""
    # åˆ›å»ºSE-Netæ¨¡å‹
    model = create_resnet_with_attention(
        num_classes=config.NUM_CLASSES,
        attention_type='se',
        reduction=16,
        dropout_rate=0.7,
        pretrained=False
    )
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # æå–æ¨¡å‹çŠ¶æ€å­—å…¸
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
        epoch = checkpoint.get('epoch', 'unknown')
    else:
        state_dict = checkpoint
        epoch = 'unknown'
    
    # åŠ è½½æƒé‡
    model.load_state_dict(state_dict)
    model.to(device)
    model.eval()
    
    print(f"SE-Netæ¨¡å‹åŠ è½½æˆåŠŸï¼Œæ¥è‡ªepoch {epoch}")
    return model


def evaluate_model(model, test_loader, device, class_names):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    all_predictions = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="è¯„ä¼°ä¸­"):
            # å¤„ç†ä¸åŒçš„æ•°æ®åŠ è½½å™¨è¿”å›æ ¼å¼
            if len(batch) == 2:
                images, labels = batch
            elif len(batch) == 3:
                images, labels, _ = batch
            else:
                raise ValueError(f"Unexpected batch format with {len(batch)} elements")
            
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    report = classification_report(
        all_labels, all_predictions, 
        target_names=class_names, 
        output_dict=True
    )
    
    # ç”Ÿæˆæ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_predictions)
    
    return {
        'accuracy': accuracy,
        'predictions': all_predictions,
        'labels': all_labels,
        'probabilities': all_probs,
        'classification_report': report,
        'confusion_matrix': cm
    }


def plot_confusion_matrix(cm, class_names, save_path):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('SE-Net æ··æ·†çŸ©é˜µ')
    plt.xlabel('é¢„æµ‹ç±»åˆ«')
    plt.ylabel('çœŸå®ç±»åˆ«')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()


def save_detailed_results(results, class_names, output_dir):
    """ä¿å­˜è¯¦ç»†ç»“æœ"""
    # ä¿å­˜åˆ†ç±»æŠ¥å‘Š
    report_path = output_dir / "classification_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(results['classification_report'], f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    predictions_df = pd.DataFrame({
        'true_label': [class_names[label] for label in results['labels']],
        'predicted_label': [class_names[pred] for pred in results['predictions']],
        'true_label_idx': results['labels'],
        'predicted_label_idx': results['predictions'],
        'correct': np.array(results['labels']) == np.array(results['predictions'])
    })
    
    # æ·»åŠ æ¦‚ç‡ä¿¡æ¯
    for i, class_name in enumerate(class_names):
        predictions_df[f'prob_{class_name}'] = [prob[i] for prob in results['probabilities']]
    
    predictions_path = output_dir / "detailed_predictions.csv"
    predictions_df.to_csv(predictions_path, index=False, encoding='utf-8')
    
    # ä¿å­˜é”™è¯¯åˆ†æ
    errors_df = predictions_df[~predictions_df['correct']].copy()
    errors_path = output_dir / "error_analysis.csv"
    errors_df.to_csv(errors_path, index=False, encoding='utf-8')
    
    return len(errors_df)


def main():
    parser = argparse.ArgumentParser(description='Evaluate SE-Net model')
    parser.add_argument('--model_path', type=str, required=True, help='Path to SE-Net checkpoint')
    parser.add_argument('--output_dir', type=str, default='outputs/evaluation/se_net_evaluation', 
                       help='Output directory for results')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(args.output_dir + f"_{timestamp}")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*60)
    print("ğŸ”¥ SE-Net æ¨¡å‹è¯„ä¼°å¼€å§‹")
    print("="*60)
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®å˜æ¢
    test_transform = transforms.Compose([
        transforms.Resize((config.INPUT_SIZE, config.INPUT_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    data_dir = Path("data/processed")
    test_df = pd.read_csv(data_dir / "test_split.csv")
    
    class_names = list(config.TARGET_CLASSES.values())
    label_to_idx = {label: idx for idx, label in enumerate(class_names)}
    
    test_dataset = TomatoSpotDataset(test_df, test_transform, label_to_idx)
    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False,
                           num_workers=config.NUM_WORKERS, pin_memory=True)
    
    print(f"æµ‹è¯•æ•°æ®é›†å¤§å°: {len(test_dataset)}")
    
    # æ‰“å°ç±»åˆ«åˆ†å¸ƒ
    test_class_counts = test_df['label'].value_counts()
    print("æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ:")
    for class_name in class_names:
        count = test_class_counts.get(class_name, 0)
        print(f"  {class_name}: {count}")
    
    # åŠ è½½æ¨¡å‹
    print("\nåŠ è½½SE-Netæ¨¡å‹...")
    model = load_se_net_model(args.model_path, device)
    
    # è¯„ä¼°æ¨¡å‹
    print("\nå¼€å§‹æ¨¡å‹è¯„ä¼°...")
    results = evaluate_model(model, test_loader, device, class_names)
    
    # ä¿å­˜ç»“æœ
    print("\nä¿å­˜è¯„ä¼°ç»“æœ...")
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    cm_path = output_dir / "confusion_matrix.png"
    plot_confusion_matrix(results['confusion_matrix'], class_names, cm_path)
    print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜: {cm_path}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    num_errors = save_detailed_results(results, class_names, output_dir)
    
    # æ‰“å°ç»“æœæ‘˜è¦
    print("\n" + "="*60)
    print("SE-Net æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("="*60)
    print(f"æ¨¡å‹: {args.model_path}")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
    print(f"é”™è¯¯åˆ†ç±»æ ·æœ¬æ•°: {num_errors}")
    
    # æ‰“å°æ¯ç±»æŒ‡æ ‡
    report = results['classification_report']
    print(f"\næ•´ä½“æŒ‡æ ‡:")
    print(f"  å®å¹³å‡ç²¾ç¡®ç‡: {report['macro avg']['precision']:.4f}")
    print(f"  å®å¹³å‡å¬å›ç‡: {report['macro avg']['recall']:.4f}")
    print(f"  å®å¹³å‡F1åˆ†æ•°: {report['macro avg']['f1-score']:.4f}")
    
    print(f"\nå„ç±»åˆ«æŒ‡æ ‡:")
    for class_name in class_names:
        if class_name in report:
            metrics = report[class_name]
            print(f"  {class_name}:")
            print(f"    ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
            print(f"    å¬å›ç‡: {metrics['recall']:.4f}")
            print(f"    F1åˆ†æ•°: {metrics['f1-score']:.4f}")
            print(f"    æ”¯æŒæ•°: {int(metrics['support'])}")
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    print("="*60)
    print("âœ… SE-Netæ¨¡å‹è¯„ä¼°å®Œæˆ!")


if __name__ == "__main__":
    main() 