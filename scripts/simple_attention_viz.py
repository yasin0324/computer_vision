#!/usr/bin/env python3
"""
ç®€åŒ–çš„æ³¨æ„åŠ›å¯è§†åŒ–è„šæœ¬
ä¸“æ³¨äºåŸºæœ¬çš„æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
"""

import os
import sys
import torch
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from torch.utils.data import DataLoader
from torchvision import transforms
from PIL import Image

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.models.attention_models import ResNetSE, ResNetCBAM
from src.data.dataset import TomatoSpotDataset
from src.config.config import Config

def load_model(model_type, checkpoint_path, config):
    """åŠ è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
    print(f"åŠ è½½{model_type}æ¨¡å‹: {checkpoint_path}")
    
    if model_type == 'se_net':
        model = ResNetSE(
            num_classes=config.NUM_CLASSES,
            reduction=16,
            dropout_rate=0.7
        )
    elif model_type == 'cbam':
        model = ResNetCBAM(
            num_classes=config.NUM_CLASSES,
            reduction=16,
            dropout_rate=0.7
        )
    else:
        raise ValueError(f"Unsupported model type: {model_type}")
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    model.model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"{model_type}æ¨¡å‹åŠ è½½æˆåŠŸ")
    return model

def extract_attention_weights(model, input_tensor):
    """æå–æ³¨æ„åŠ›æƒé‡"""
    attention_weights = {}
    
    def hook_fn(name):
        def hook(module, input, output):
            # å¯¹äºSEæ¨¡å—ï¼Œç›´æ¥ä¿å­˜è¾“å‡ºä½œä¸ºæ³¨æ„åŠ›æƒé‡
            if 'se' in name.lower():
                attention_weights[name] = output.detach().cpu()
            # å¯¹äºCBAMæ¨¡å—ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            elif hasattr(module, 'channel_attention') or hasattr(module, 'spatial_attention'):
                attention_weights[name] = output.detach().cpu()
        return hook
    
    # æ³¨å†Œé’©å­
    hooks = []
    if hasattr(model, 'model'):
        base_model = model.model
    else:
        base_model = model
    
    # ä¸ºå…³é”®å±‚æ³¨å†Œé’©å­
    for name, module in base_model.named_modules():
        if 'se' in name or 'cbam' in name or 'attention' in name:
            hook = module.register_forward_hook(hook_fn(name))
            hooks.append(hook)
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(input_tensor)
        pred_probs = torch.softmax(output, dim=1)
        pred_class = torch.argmax(output, dim=1).item()
    
    # æ¸…ç†é’©å­
    for hook in hooks:
        hook.remove()
    
    return attention_weights, pred_probs, pred_class

def visualize_sample_simple(models, sample_data, class_names, output_dir, sample_idx):
    """ç®€åŒ–çš„æ ·æœ¬å¯è§†åŒ–"""
    image, label, image_path = sample_data
    
    # ç¡®ä¿image_pathæ˜¯å­—ç¬¦ä¸²
    if not isinstance(image_path, str):
        image_path = f"sample_{sample_idx}"
    
    sample_dir = Path(output_dir) / f"sample_{sample_idx}_simple"
    sample_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜åŸå§‹å›¾åƒ
    save_original_image(image, sample_dir)
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹æå–å’Œå¯è§†åŒ–æ³¨æ„åŠ›
    results = {}
    
    for model_name, model in models.items():
        print(f"  å¤„ç† {model_name} æ¨¡å‹...")
        
        # æå–æ³¨æ„åŠ›æƒé‡
        attention_weights, pred_probs, pred_class = extract_attention_weights(model, image)
        
        results[model_name] = {
            'attention_weights': attention_weights,
            'pred_probs': pred_probs,
            'pred_class': pred_class
        }
        
        # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
        model_dir = sample_dir / model_name
        model_dir.mkdir(exist_ok=True)
        
        visualize_attention_weights(attention_weights, model_dir, model_name)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    generate_simple_report(results, label.item(), class_names, sample_dir, image_path)
    
    print(f"æ ·æœ¬ {sample_idx} ç®€åŒ–å¯è§†åŒ–å®Œæˆ: {sample_dir}")
    return sample_dir

def save_original_image(image_tensor, output_dir):
    """ä¿å­˜åŸå§‹å›¾åƒ"""
    # åæ ‡å‡†åŒ–
    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
    
    image = image_tensor.squeeze(0) * std + mean
    image = torch.clamp(image, 0, 1)
    
    # è½¬æ¢ä¸ºPILå›¾åƒå¹¶ä¿å­˜
    image_np = image.permute(1, 2, 0).numpy()
    image_pil = Image.fromarray((image_np * 255).astype(np.uint8))
    image_pil.save(output_dir / "original_image.png")

def visualize_attention_weights(attention_weights, output_dir, model_name):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    if not attention_weights:
        print(f"    {model_name}: æ²¡æœ‰æ‰¾åˆ°æ³¨æ„åŠ›æƒé‡")
        return
    
    # åˆ›å»ºå›¾åƒ
    num_weights = len(attention_weights)
    if num_weights == 0:
        return
    
    fig, axes = plt.subplots(1, min(num_weights, 4), figsize=(15, 4))
    if num_weights == 1:
        axes = [axes]
    elif num_weights > 4:
        axes = axes[:4]
        attention_weights = dict(list(attention_weights.items())[:4])
    
    for idx, (name, weights) in enumerate(attention_weights.items()):
        ax = axes[idx] if num_weights > 1 else axes[0]
        
        # å¤„ç†ä¸åŒå½¢çŠ¶çš„æƒé‡
        if weights.dim() == 4:  # [B, C, H, W]
            if weights.shape[2] == 1 and weights.shape[3] == 1:  # é€šé“æ³¨æ„åŠ›
                weights_1d = weights.squeeze().numpy()
                bars = ax.bar(range(len(weights_1d)), weights_1d)
                ax.set_title(f'{name}\nChannel Attention')
                ax.set_xlabel('Channel Index')
                ax.set_ylabel('Weight')
                
                # é«˜äº®æœ€é‡è¦çš„é€šé“
                if len(weights_1d) > 5:
                    max_indices = np.argsort(weights_1d)[-5:]
                    for i in max_indices:
                        bars[i].set_color('red')
            else:  # ç©ºé—´æ³¨æ„åŠ›æˆ–ç‰¹å¾å›¾
                # å¦‚æœæ˜¯3Dï¼Œå–ç¬¬ä¸€ä¸ªé€šé“æˆ–å¹³å‡
                if weights.dim() == 4 and weights.shape[1] > 1:
                    weights_2d = weights.squeeze(0).mean(dim=0).numpy()
                else:
                    weights_2d = weights.squeeze().numpy()
                    
                # ç¡®ä¿æ˜¯2D
                if weights_2d.ndim > 2:
                    weights_2d = weights_2d.mean(axis=0)
                
                im = ax.imshow(weights_2d, cmap='jet')
                ax.set_title(f'{name}\nSpatial Attention')
                plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
                ax.axis('off')
        else:
            # å…¶ä»–æƒ…å†µï¼Œå°è¯•å±•å¹³å¹¶æ˜¾ç¤º
            weights_flat = weights.flatten().numpy()
            ax.plot(weights_flat)
            ax.set_title(f'{name}\nWeight Distribution')
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(len(attention_weights), len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    plt.savefig(output_dir / f"{model_name}_attention_weights.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"    {model_name}: ä¿å­˜äº† {len(attention_weights)} ä¸ªæ³¨æ„åŠ›æƒé‡å›¾")

def generate_simple_report(results, true_label, class_names, output_dir, image_path):
    """ç”Ÿæˆç®€åŒ–æŠ¥å‘Š"""
    report_path = output_dir / "simple_analysis.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ç®€åŒ–æ³¨æ„åŠ›åˆ†ææŠ¥å‘Š\n\n")
        
        f.write("## æ ·æœ¬ä¿¡æ¯\n")
        f.write(f"- å›¾åƒè·¯å¾„: {image_path}\n")
        f.write(f"- çœŸå®æ ‡ç­¾: {class_names[true_label]}\n\n")
        
        f.write("## æ¨¡å‹é¢„æµ‹å¯¹æ¯”\n")
        f.write("| æ¨¡å‹ | é¢„æµ‹ç±»åˆ« | ç½®ä¿¡åº¦ | é¢„æµ‹æ­£ç¡® |\n")
        f.write("|------|----------|--------|----------|\n")
        
        for model_name, result in results.items():
            pred_class = result['pred_class']
            confidence = result['pred_probs'][0][pred_class].item()
            pred_class_name = class_names[pred_class]
            is_correct = "âœ…" if pred_class == true_label else "âŒ"
            f.write(f"| {model_name} | {pred_class_name} | {confidence:.4f} | {is_correct} |\n")
        
        f.write("\n## è¯¦ç»†æ¦‚ç‡åˆ†å¸ƒ\n")
        for model_name, result in results.items():
            f.write(f"\n### {model_name}\n")
            pred_probs = result['pred_probs'][0]
            for i, class_name in enumerate(class_names):
                prob = pred_probs[i].item()
                marker = "**" if i == true_label else ""
                f.write(f"- {marker}{class_name}{marker}: {prob:.4f} ({prob*100:.2f}%)\n")
        
        f.write("\n## æ³¨æ„åŠ›æƒé‡ç»Ÿè®¡\n")
        for model_name, result in results.items():
            f.write(f"\n### {model_name}\n")
            attention_weights = result['attention_weights']
            if attention_weights:
                f.write(f"- æ£€æµ‹åˆ° {len(attention_weights)} ä¸ªæ³¨æ„åŠ›æ¨¡å—\n")
                for name, weights in attention_weights.items():
                    f.write(f"  - {name}: å½¢çŠ¶ {list(weights.shape)}\n")
            else:
                f.write("- æœªæ£€æµ‹åˆ°æ³¨æ„åŠ›æƒé‡\n")

def main():
    parser = argparse.ArgumentParser(description='ç®€åŒ–æ³¨æ„åŠ›å¯è§†åŒ–')
    parser.add_argument('--se_net_path', type=str,
                       default='outputs/models/resnet50_se_net_from_baseline/best_checkpoint_epoch_20.pth',
                       help='SE-Netæ¨¡å‹è·¯å¾„')
    parser.add_argument('--cbam_path', type=str,
                       default='outputs/models/resnet50_cbam_from_baseline/best_checkpoint_epoch_14.pth',
                       help='CBAMæ¨¡å‹è·¯å¾„')
    parser.add_argument('--output_dir', type=str,
                       default='outputs/simple_attention_viz',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--sample_idx', type=int, default=0,
                       help='æ ·æœ¬ç´¢å¼•')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¨ ç®€åŒ–æ³¨æ„åŠ›å¯è§†åŒ–åˆ†æ")
    print("=" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åŠ è½½é…ç½®
    config = Config()
    
    # åŠ è½½æ¨¡å‹
    models = {}
    
    if os.path.exists(args.se_net_path):
        models['SE-Net'] = load_model('se_net', args.se_net_path, config)
    
    if os.path.exists(args.cbam_path):
        models['CBAM'] = load_model('cbam', args.cbam_path, config)
    
    if not models:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        return
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_transform = transforms.Compose([
        transforms.Resize((config.INPUT_SIZE, config.INPUT_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    data_dir = Path("data/processed")
    test_df = pd.read_csv(data_dir / "test_split.csv")
    
    class_names = list(config.TARGET_CLASSES.values())
    label_to_idx = {label: idx for idx, label in enumerate(class_names)}
    
    test_dataset = TomatoSpotDataset(test_df, test_transform, label_to_idx)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)
    
    # è·å–æŒ‡å®šæ ·æœ¬
    for i, batch in enumerate(test_loader):
        if i == args.sample_idx:
            if len(batch) == 2:
                image, label = batch
                image_path = f"sample_{args.sample_idx}"
            else:
                image, label, image_path = batch
            break
    else:
        print(f"æ ·æœ¬ {args.sample_idx} ä¸å­˜åœ¨")
        return
    
    print(f"å¤„ç†æ ·æœ¬ {args.sample_idx}: {class_names[label.item()]}")
    
    # å¯è§†åŒ–æ ·æœ¬
    visualize_sample_simple(models, (image, label, image_path), class_names, 
                           args.output_dir, args.sample_idx)
    
    print(f"\nğŸ‰ ç®€åŒ–å¯è§†åŒ–å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.output_dir}")

if __name__ == "__main__":
    main() 