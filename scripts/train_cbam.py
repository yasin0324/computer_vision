#!/usr/bin/env python3
"""
CBAMæ³¨æ„åŠ›æœºåˆ¶è®­ç»ƒè„šæœ¬
åŸºäºSE-Netçš„æˆåŠŸç»éªŒï¼Œå®ç°CBAMï¼ˆConvolutional Block Attention Moduleï¼‰
"""

import sys
import os
import json
import argparse
import logging
from pathlib import Path
from datetime import datetime

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
import pandas as pd
from torchvision import transforms

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.models.attention_models import create_resnet_with_attention, get_model_info
from src.data.dataset import TomatoSpotDataset
from src.training.trainer import Trainer
from src.config.config import config


def load_baseline_weights_to_cbam_model(cbam_model, baseline_checkpoint_path):
    """
    å°†åŸºçº¿æ¨¡å‹çš„æƒé‡åŠ è½½åˆ°CBAMæ¨¡å‹ä¸­
    
    Args:
        cbam_model: CBAMæ¨¡å‹
        baseline_checkpoint_path: åŸºçº¿æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
    """
    # åŠ è½½åŸºçº¿æ£€æŸ¥ç‚¹
    checkpoint = torch.load(baseline_checkpoint_path, map_location='cpu')
    baseline_state_dict = checkpoint['model_state_dict']
    
    # è·å–CBAMæ¨¡å‹çš„çŠ¶æ€å­—å…¸
    cbam_state_dict = cbam_model.state_dict()
    
    # æ˜ å°„åŸºçº¿æ¨¡å‹çš„æƒé‡åˆ°CBAMæ¨¡å‹
    weight_mapping = {}
    
    # æ˜ å°„backboneçš„æƒé‡
    for key in baseline_state_dict:
        if key.startswith('backbone.'):
            # ç§»é™¤backboneå‰ç¼€
            new_key = key.replace('backbone.', '')
            
            # å¤„ç†ç¬¬ä¸€å±‚å·ç§¯å’ŒBN
            if new_key.startswith('0.'):
                new_key = new_key.replace('0.', 'conv1.')
            elif new_key.startswith('1.'):
                new_key = new_key.replace('1.', 'bn1.')
            # å¤„ç†ResNetå±‚
            elif new_key.startswith('4.'):
                new_key = new_key.replace('4.', 'layer1.')
            elif new_key.startswith('5.'):
                new_key = new_key.replace('5.', 'layer2.')
            elif new_key.startswith('6.'):
                new_key = new_key.replace('6.', 'layer3.')
            elif new_key.startswith('7.'):
                new_key = new_key.replace('7.', 'layer4.')
            
            weight_mapping[new_key] = baseline_state_dict[key]
    
    # åŠ è½½æƒé‡åˆ°CBAMæ¨¡å‹ï¼ˆå¿½ç•¥CBAMç›¸å…³çš„å‚æ•°å’Œåˆ†ç±»å™¨ï¼‰
    missing_keys = []
    loaded_keys = []
    
    for key in weight_mapping:
        if key in cbam_state_dict:
            # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
            if weight_mapping[key].shape == cbam_state_dict[key].shape:
                cbam_state_dict[key] = weight_mapping[key]
                loaded_keys.append(key)
            else:
                missing_keys.append(f"{key} (shape mismatch: {weight_mapping[key].shape} vs {cbam_state_dict[key].shape})")
        else:
            missing_keys.append(key)
    
    # åŠ è½½æƒé‡
    cbam_model.load_state_dict(cbam_state_dict, strict=False)
    
    print(f"æˆåŠŸåŠ è½½åŸºçº¿æ¨¡å‹æƒé‡")
    print(f"å·²åŠ è½½çš„æƒé‡: {len(loaded_keys)} ä¸ª")
    if missing_keys:
        print(f"è·³è¿‡çš„æƒé‡: {len(missing_keys)} ä¸ª")
    print("æ³¨æ„: CBAMæ¨¡å—å’Œåˆ†ç±»å™¨æƒé‡å°†ä»å¤´å¼€å§‹è®­ç»ƒ")
    
    return cbam_model


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Train CBAM attention model')
    
    # CBAMç‰¹å®šå‚æ•°
    parser.add_argument('--reduction', type=int, default=16, help='CBAM reduction ratio')
    parser.add_argument('--dropout_rate', type=float, default=0.7, help='Dropout rate')
    parser.add_argument('--epochs', type=int, default=30, help='Number of epochs')
    parser.add_argument('--learning_rate', type=float, default=0.0005, help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=0.001, help='Weight decay')
    parser.add_argument('--patience', type=int, default=8, help='Early stopping patience')
    parser.add_argument('--experiment_name', type=str, default='resnet50_cbam')
    parser.add_argument('--load_baseline', type=str, default=None, 
                       help='Path to baseline checkpoint to initialize from')
    
    args = parser.parse_args()
    
    print("="*60)
    print("ğŸ”¥ CBAM æ³¨æ„åŠ›æœºåˆ¶è®­ç»ƒå¼€å§‹")
    print("="*60)
    print(f"å®éªŒé…ç½®:")
    print(f"  - æ³¨æ„åŠ›ç±»å‹: CBAM (Convolutional Block Attention Module)")
    print(f"  - é™ç»´æ¯”ä¾‹: {args.reduction}")
    print(f"  - Dropoutç‡: {args.dropout_rate}")
    print(f"  - è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"  - å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"  - æƒé‡è¡°å‡: {args.weight_decay}")
    print(f"  - æ—©åœpatience: {args.patience}")
    print(f"  - å®éªŒåç§°: {args.experiment_name}")
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"  - ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®å¢å¼ºï¼ˆä¸SE-Netä¿æŒä¸€è‡´ï¼‰
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomResizedCrop(config.INPUT_SIZE, scale=(0.7, 1.0)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.3),
        transforms.RandomRotation(degrees=30),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.15),
        transforms.RandomGrayscale(p=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((config.INPUT_SIZE, config.INPUT_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åŠ è½½æ•°æ®
    data_dir = Path("data/processed")
    train_df = pd.read_csv(data_dir / "train_split.csv")
    val_df = pd.read_csv(data_dir / "val_split.csv")
    
    class_names = list(config.TARGET_CLASSES.values())
    label_to_idx = {label: idx for idx, label in enumerate(class_names)}
    
    train_dataset = TomatoSpotDataset(train_df, train_transform, label_to_idx)
    val_dataset = TomatoSpotDataset(val_df, val_transform, label_to_idx)
    
    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, 
                             num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False,
                           num_workers=config.NUM_WORKERS, pin_memory=True)
    
    print(f"æ•°æ®åŠ è½½å®Œæˆ: {len(train_dataset)} è®­ç»ƒæ ·æœ¬, {len(val_dataset)} éªŒè¯æ ·æœ¬")
    
    # åˆ›å»ºCBAMæ¨¡å‹
    print("åˆ›å»ºCBAMæ¨¡å‹...")
    model = create_resnet_with_attention(
        num_classes=config.NUM_CLASSES,
        attention_type='cbam',
        reduction=args.reduction,
        dropout_rate=args.dropout_rate,
        pretrained=True  # å…ˆä½¿ç”¨é¢„è®­ç»ƒæƒé‡
    )
    
    # å¦‚æœæŒ‡å®šäº†åŸºçº¿æ¨¡å‹ï¼Œåˆ™åŠ è½½å…¶æƒé‡
    if args.load_baseline:
        print(f"ä»åŸºçº¿æ¨¡å‹åˆå§‹åŒ–æƒé‡: {args.load_baseline}")
        model = load_baseline_weights_to_cbam_model(model, args.load_baseline)
    else:
        print("ä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–")
    
    model.to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    model_info = get_model_info(model)
    print(f"\næ¨¡å‹ä¿¡æ¯:")
    print(f"  - æ€»å‚æ•°æ•°: {model_info['total_parameters']:,}")
    print(f"  - å¯è®­ç»ƒå‚æ•°: {model_info['trainable_parameters']:,}")
    print(f"  - CBAMæ¨¡å—æ•°é‡: {model_info['cbam_blocks']}")
    print(f"  - æ¨¡å‹å¤§å°: {model_info['model_size_mb']:.2f} MB")
    
    # è®­ç»ƒç»„ä»¶ï¼ˆä¸SE-Netä¿æŒä¸€è‡´ï¼‰
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # æ ‡ç­¾å¹³æ»‘
    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate, 
                           weight_decay=args.weight_decay)  # AdamW
    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        experiment_name=args.experiment_name
    )
    
    # åˆ›å»ºæ—©åœæœºåˆ¶
    from src.training.utils import EarlyStopping
    early_stopping = EarlyStopping(
        patience=args.patience,
        min_delta=0.005
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹CBAMè®­ç»ƒ...")
    print("="*60)
    trainer.train(epochs=args.epochs, early_stopping=early_stopping)
    
    print("\nğŸ‰ CBAMè®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {trainer.best_val_acc:.4f}%")
    
    # ä¿å­˜æ¨¡å‹ä¿¡æ¯
    info_path = Path(f"outputs/models/{args.experiment_name}/model_info.json")
    with open(info_path, 'w') as f:
        json.dump(model_info, f, indent=2)
    
    print(f"æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")


if __name__ == "__main__":
    main() 