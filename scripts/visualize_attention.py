#!/usr/bin/env python3
"""
æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–åˆ†æè„šæœ¬
å¯¹æ¯”SE-Netå’ŒCBAMçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”Ÿæˆå¯è§†åŒ–åˆ†ææŠ¥å‘Š
"""

import os
import sys
import torch
import argparse
import pandas as pd
from pathlib import Path
from torch.utils.data import DataLoader
from torchvision import transforms

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.models.attention_models import ResNetSE, ResNetCBAM
from src.data.dataset import TomatoSpotDataset
from src.config.config import Config
from src.visualization.attention_visualizer import AttentionVisualizer
from src.visualization.grad_cam import visualize_grad_cam

def load_model(model_type, checkpoint_path, config):
    """åŠ è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
    print(f"åŠ è½½{model_type}æ¨¡å‹: {checkpoint_path}")
    
    if model_type == 'se_net':
        model = ResNetSE(
            num_classes=config.NUM_CLASSES,
            reduction=16,
            dropout_rate=0.7
        )
    elif model_type == 'cbam':
        model = ResNetCBAM(
            num_classes=config.NUM_CLASSES,
            reduction=16,
            dropout_rate=0.7
        )
    else:
        raise ValueError(f"Unsupported model type: {model_type}")
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    model.model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"{model_type}æ¨¡å‹åŠ è½½æˆåŠŸ")
    return model

def create_test_loader(config):
    """åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    # æ•°æ®å˜æ¢
    test_transform = transforms.Compose([
        transforms.Resize((config.INPUT_SIZE, config.INPUT_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    data_dir = Path("data/processed")
    test_df = pd.read_csv(data_dir / "test_split.csv")
    
    class_names = list(config.TARGET_CLASSES.values())
    label_to_idx = {label: idx for idx, label in enumerate(class_names)}
    
    test_dataset = TomatoSpotDataset(test_df, test_transform, label_to_idx)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                           num_workers=0, pin_memory=False)  # å•æ ·æœ¬å¤„ç†
    
    return test_loader, class_names

def visualize_single_sample(models, test_loader, class_names, output_dir, sample_idx=0):
    """å¯è§†åŒ–å•ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”"""
    print(f"\nğŸ” å¯è§†åŒ–æ ·æœ¬ {sample_idx}")
    
    # è·å–æŒ‡å®šæ ·æœ¬
    for i, batch in enumerate(test_loader):
        if i == sample_idx:
            if len(batch) == 2:
                image, label = batch
                image_path = f"sample_{sample_idx}"
            else:
                image, label, image_path = batch
            break
    else:
        print(f"æ ·æœ¬ {sample_idx} ä¸å­˜åœ¨")
        return
    
    sample_dir = Path(output_dir) / f"sample_{sample_idx}_comparison"
    sample_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜åŸå§‹å›¾åƒä¿¡æ¯
    class_name = class_names[label.item()]
    print(f"æ ·æœ¬ç±»åˆ«: {class_name}")
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆå¯è§†åŒ–
    for model_name, model in models.items():
        print(f"  å¤„ç† {model_name} æ¨¡å‹...")
        
        model_dir = sample_dir / model_name
        model_dir.mkdir(exist_ok=True)
        
        # æ³¨æ„åŠ›å¯è§†åŒ–
        visualizer = AttentionVisualizer(model, device='cpu')
        try:
            visualizer.visualize_sample(image, label.item(), image_path, model_dir)
        except Exception as e:
            print(f"    æ³¨æ„åŠ›å¯è§†åŒ–å¤±è´¥: {e}")
        finally:
            visualizer.cleanup()
        
        # Grad-CAMå¯è§†åŒ–
        try:
            from src.visualization.grad_cam import GradCAM
            grad_cam = GradCAM(model, 'layer4', device='cpu')
            
            # ä¸ºçœŸå®ç±»åˆ«ç”ŸæˆCAM
            cam_path = model_dir / "grad_cam_true_class.png"
            grad_cam.save_visualization(image, label.item(), cam_path, class_names)
            
            # ä¸ºé¢„æµ‹ç±»åˆ«ç”ŸæˆCAM
            with torch.no_grad():
                pred = model(image)
                pred_class = torch.argmax(pred, dim=1).item()
            
            if pred_class != label.item():
                cam_path = model_dir / "grad_cam_pred_class.png"
                grad_cam.save_visualization(image, pred_class, cam_path, class_names)
            
        except Exception as e:
            print(f"    Grad-CAMå¯è§†åŒ–å¤±è´¥: {e}")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    generate_comparison_report(models, image, label.item(), class_names, sample_dir)
    
    print(f"æ ·æœ¬ {sample_idx} å¯è§†åŒ–å®Œæˆ: {sample_dir}")

def generate_comparison_report(models, image, true_label, class_names, output_dir):
    """ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š"""
    report_path = output_dir / "comparison_report.md"
    
    # è·å–æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹
    predictions = {}
    for model_name, model in models.items():
        with torch.no_grad():
            pred = model(image)
            pred_probs = torch.softmax(pred, dim=1)[0]
            pred_class = torch.argmax(pred, dim=1).item()
            predictions[model_name] = {
                'class': pred_class,
                'confidence': pred_probs[pred_class].item(),
                'probabilities': pred_probs
            }
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”åˆ†ææŠ¥å‘Š\n\n")
        
        f.write("## æ ·æœ¬ä¿¡æ¯\n")
        f.write(f"- çœŸå®æ ‡ç­¾: {class_names[true_label]}\n")
        f.write(f"- å›¾åƒå°ºå¯¸: {image.shape}\n\n")
        
        f.write("## æ¨¡å‹é¢„æµ‹å¯¹æ¯”\n")
        f.write("| æ¨¡å‹ | é¢„æµ‹ç±»åˆ« | ç½®ä¿¡åº¦ | é¢„æµ‹æ­£ç¡® |\n")
        f.write("|------|----------|--------|----------|\n")
        
        for model_name, pred_info in predictions.items():
            pred_class_name = class_names[pred_info['class']]
            confidence = pred_info['confidence']
            is_correct = "âœ…" if pred_info['class'] == true_label else "âŒ"
            f.write(f"| {model_name} | {pred_class_name} | {confidence:.4f} | {is_correct} |\n")
        
        f.write("\n## è¯¦ç»†æ¦‚ç‡åˆ†å¸ƒ\n")
        for model_name, pred_info in predictions.items():
            f.write(f"\n### {model_name}\n")
            for i, class_name in enumerate(class_names):
                prob = pred_info['probabilities'][i].item()
                marker = "**" if i == true_label else ""
                f.write(f"- {marker}{class_name}{marker}: {prob:.4f} ({prob*100:.2f}%)\n")
        
        f.write("\n## æ³¨æ„åŠ›æœºåˆ¶åˆ†æ\n")
        f.write("### SE-Net (é€šé“æ³¨æ„åŠ›)\n")
        f.write("- å…³æ³¨ç‰¹å¾é€šé“çš„é‡è¦æ€§\n")
        f.write("- å­¦ä¹ 'å…³æ³¨ä»€ä¹ˆ'ç‰¹å¾\n")
        f.write("- å‚æ•°è¾ƒå°‘ï¼Œè®¡ç®—é«˜æ•ˆ\n\n")
        
        f.write("### CBAM (åŒé‡æ³¨æ„åŠ›)\n")
        f.write("- ç»“åˆé€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›\n")
        f.write("- å­¦ä¹ 'å…³æ³¨ä»€ä¹ˆ'å’Œ'å…³æ³¨å“ªé‡Œ'\n")
        f.write("- æ›´å…¨é¢çš„æ³¨æ„åŠ›æœºåˆ¶\n\n")
        
        f.write("## å¯è§†åŒ–æ–‡ä»¶è¯´æ˜\n")
        f.write("- `original_image.png`: åŸå§‹è¾“å…¥å›¾åƒ\n")
        f.write("- `channel_attention.png`: é€šé“æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ\n")
        f.write("- `spatial_attention.png`: ç©ºé—´æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼ˆä»…CBAMï¼‰\n")
        f.write("- `feature_maps.png`: ç‰¹å¾å›¾å¯è§†åŒ–\n")
        f.write("- `grad_cam_*.png`: Grad-CAMç±»æ¿€æ´»å›¾\n")
        f.write("- `attention_analysis.md`: è¯¦ç»†æ³¨æ„åŠ›åˆ†æ\n")

def batch_visualize_attention(models, test_loader, class_names, output_dir, num_samples_per_class=2):
    """æ‰¹é‡å¯è§†åŒ–ä¸åŒç±»åˆ«çš„æ³¨æ„åŠ›æœºåˆ¶"""
    print(f"\nğŸ“Š æ‰¹é‡å¯è§†åŒ–æ³¨æ„åŠ›æœºåˆ¶")
    
    # ä¸ºæ¯ä¸ªç±»åˆ«æ”¶é›†æ ·æœ¬
    class_samples = {i: [] for i in range(len(class_names))}
    
    for batch in test_loader:
        if len(batch) == 2:
            image, label = batch
            image_path = f"sample_{len(class_samples[label.item()])}"
        else:
            image, label, image_path = batch
        
        label_idx = label.item()
        if len(class_samples[label_idx]) < num_samples_per_class:
            class_samples[label_idx].append((image, label_idx, image_path))
        
        # æ£€æŸ¥æ˜¯å¦æ”¶é›†å¤Ÿäº†æ ·æœ¬
        if all(len(samples) >= num_samples_per_class for samples in class_samples.values()):
            break
    
    # ä¸ºæ¯ä¸ªç±»åˆ«å’Œæ¨¡å‹ç”Ÿæˆå¯è§†åŒ–
    for class_idx, class_name in enumerate(class_names):
        if not class_samples[class_idx]:
            continue
            
        print(f"\nå¤„ç†ç±»åˆ«: {class_name}")
        class_dir = Path(output_dir) / f"class_{class_name}"
        class_dir.mkdir(parents=True, exist_ok=True)
        
        for sample_idx, (image, label, image_path) in enumerate(class_samples[class_idx]):
            sample_dir = class_dir / f"sample_{sample_idx}"
            sample_dir.mkdir(exist_ok=True)
            
            for model_name, model in models.items():
                print(f"  {model_name} - æ ·æœ¬ {sample_idx}")
                
                model_dir = sample_dir / model_name
                model_dir.mkdir(exist_ok=True)
                
                # æ³¨æ„åŠ›å¯è§†åŒ–
                visualizer = AttentionVisualizer(model, device='cpu')
                try:
                    visualizer.visualize_sample(image, label, image_path, model_dir)
                except Exception as e:
                    print(f"    æ³¨æ„åŠ›å¯è§†åŒ–å¤±è´¥: {e}")
                finally:
                    visualizer.cleanup()

def main():
    parser = argparse.ArgumentParser(description='æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–åˆ†æ')
    parser.add_argument('--se_net_path', type=str,
                       default='outputs/models/resnet50_se_net_from_baseline/best_checkpoint_epoch_20.pth',
                       help='SE-Netæ¨¡å‹è·¯å¾„')
    parser.add_argument('--cbam_path', type=str,
                       default='outputs/models/resnet50_cbam_from_baseline/best_checkpoint_epoch_14.pth',
                       help='CBAMæ¨¡å‹è·¯å¾„')
    parser.add_argument('--output_dir', type=str,
                       default='outputs/attention_visualization',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--mode', type=str, choices=['single', 'batch', 'both'], default='both',
                       help='å¯è§†åŒ–æ¨¡å¼')
    parser.add_argument('--sample_idx', type=int, default=0,
                       help='å•æ ·æœ¬æ¨¡å¼ä¸‹çš„æ ·æœ¬ç´¢å¼•')
    parser.add_argument('--num_samples', type=int, default=2,
                       help='æ‰¹é‡æ¨¡å¼ä¸‹æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¨ æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–åˆ†æ")
    print("=" * 60)
    print(f"SE-Netæ¨¡å‹: {args.se_net_path}")
    print(f"CBAMæ¨¡å‹: {args.cbam_path}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"å¯è§†åŒ–æ¨¡å¼: {args.mode}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åŠ è½½é…ç½®
    config = Config()
    
    # åŠ è½½æ¨¡å‹
    models = {}
    
    if os.path.exists(args.se_net_path):
        models['SE-Net'] = load_model('se_net', args.se_net_path, config)
    else:
        print(f"âš ï¸ SE-Netæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.se_net_path}")
    
    if os.path.exists(args.cbam_path):
        models['CBAM'] = load_model('cbam', args.cbam_path, config)
    else:
        print(f"âš ï¸ CBAMæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.cbam_path}")
    
    if not models:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œé€€å‡º")
        return
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_loader, class_names = create_test_loader(config)
    print(f"æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆï¼Œç±»åˆ«: {class_names}")
    
    # æ‰§è¡Œå¯è§†åŒ–
    if args.mode in ['single', 'both']:
        print(f"\nğŸ” å•æ ·æœ¬å¯è§†åŒ–åˆ†æ")
        visualize_single_sample(models, test_loader, class_names, 
                               args.output_dir, args.sample_idx)
    
    if args.mode in ['batch', 'both']:
        print(f"\nğŸ“Š æ‰¹é‡å¯è§†åŒ–åˆ†æ")
        batch_visualize_attention(models, test_loader, class_names,
                                 args.output_dir, args.num_samples)
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    generate_summary_report(models, class_names, args.output_dir)
    
    print(f"\nğŸ‰ æ³¨æ„åŠ›å¯è§†åŒ–åˆ†æå®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åœ¨: {args.output_dir}")

def generate_summary_report(models, class_names, output_dir):
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    report_path = Path(output_dir) / "visualization_summary.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–åˆ†ææ€»ç»“\n\n")
        
        f.write("## å®éªŒæ¦‚è¿°\n")
        f.write("æœ¬å®éªŒå¯¹æ¯”åˆ†æäº†SE-Netå’ŒCBAMä¸¤ç§æ³¨æ„åŠ›æœºåˆ¶åœ¨æ¤ç‰©å¶ç‰‡ç—…å®³è¯†åˆ«ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚\n\n")
        
        f.write("## æ¨¡å‹ä¿¡æ¯\n")
        for model_name in models.keys():
            f.write(f"- **{model_name}**: ResNet50 + {model_name}æ³¨æ„åŠ›æœºåˆ¶\n")
        f.write(f"- **æ•°æ®é›†**: ç•ªèŒ„å¶æ–‘ç—…ç»†ç²’åº¦è¯†åˆ«ï¼ˆ{len(class_names)}ç±»ï¼‰\n")
        f.write(f"- **ç±»åˆ«**: {', '.join(class_names)}\n\n")
        
        f.write("## æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”\n")
        f.write("### SE-Net (Squeeze-and-Excitation)\n")
        f.write("- **æœºåˆ¶**: é€šé“æ³¨æ„åŠ›\n")
        f.write("- **åŸç†**: é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–å’Œå…¨è¿æ¥å±‚å­¦ä¹ é€šé“æƒé‡\n")
        f.write("- **ä¼˜åŠ¿**: å‚æ•°å°‘ï¼Œè®¡ç®—é«˜æ•ˆï¼Œå…³æ³¨é‡è¦ç‰¹å¾é€šé“\n")
        f.write("- **å¯è§†åŒ–**: é€šé“æƒé‡åˆ†å¸ƒå›¾\n\n")
        
        f.write("### CBAM (Convolutional Block Attention Module)\n")
        f.write("- **æœºåˆ¶**: é€šé“æ³¨æ„åŠ› + ç©ºé—´æ³¨æ„åŠ›\n")
        f.write("- **åŸç†**: å…ˆå­¦ä¹ é€šé“æƒé‡ï¼Œå†å­¦ä¹ ç©ºé—´ä½ç½®æƒé‡\n")
        f.write("- **ä¼˜åŠ¿**: æ›´å…¨é¢çš„æ³¨æ„åŠ›ï¼ŒåŒæ—¶å…³æ³¨'ä»€ä¹ˆ'å’Œ'å“ªé‡Œ'\n")
        f.write("- **å¯è§†åŒ–**: é€šé“æƒé‡åˆ†å¸ƒå›¾ + ç©ºé—´æ³¨æ„åŠ›çƒ­åŠ›å›¾\n\n")
        
        f.write("## å¯è§†åŒ–å†…å®¹è¯´æ˜\n")
        f.write("### æ–‡ä»¶ç»“æ„\n")
        f.write("```\n")
        f.write("attention_visualization/\n")
        f.write("â”œâ”€â”€ sample_X_comparison/          # å•æ ·æœ¬å¯¹æ¯”åˆ†æ\n")
        f.write("â”‚   â”œâ”€â”€ SE-Net/                   # SE-Netå¯è§†åŒ–ç»“æœ\n")
        f.write("â”‚   â”œâ”€â”€ CBAM/                     # CBAMå¯è§†åŒ–ç»“æœ\n")
        f.write("â”‚   â””â”€â”€ comparison_report.md      # å¯¹æ¯”åˆ†ææŠ¥å‘Š\n")
        f.write("â”œâ”€â”€ class_XXX/                    # å„ç±»åˆ«æ‰¹é‡åˆ†æ\n")
        f.write("â”‚   â””â”€â”€ sample_X/\n")
        f.write("â”‚       â”œâ”€â”€ SE-Net/\n")
        f.write("â”‚       â””â”€â”€ CBAM/\n")
        f.write("â””â”€â”€ visualization_summary.md      # æ€»ç»“æŠ¥å‘Š\n")
        f.write("```\n\n")
        
        f.write("### å¯è§†åŒ–å›¾åƒè¯´æ˜\n")
        f.write("- **original_image.png**: åŸå§‹è¾“å…¥å›¾åƒ\n")
        f.write("- **channel_attention.png**: é€šé“æ³¨æ„åŠ›æƒé‡æŸ±çŠ¶å›¾\n")
        f.write("- **spatial_attention.png**: ç©ºé—´æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼ˆä»…CBAMï¼‰\n")
        f.write("- **feature_maps.png**: ä¸åŒå±‚çš„ç‰¹å¾å›¾å¯è§†åŒ–\n")
        f.write("- **grad_cam_*.png**: Grad-CAMç±»æ¿€æ´»å›¾\n")
        f.write("- **attention_analysis.md**: è¯¦ç»†æ•°å€¼åˆ†æ\n\n")
        
        f.write("## åˆ†æè¦ç‚¹\n")
        f.write("1. **é€šé“æ³¨æ„åŠ›å¯¹æ¯”**: è§‚å¯ŸSE-Netå’ŒCBAMçš„é€šé“æƒé‡åˆ†å¸ƒå·®å¼‚\n")
        f.write("2. **ç©ºé—´æ³¨æ„åŠ›åˆ†æ**: CBAMçš„ç©ºé—´æ³¨æ„åŠ›å¦‚ä½•å®šä½å…³é”®åŒºåŸŸ\n")
        f.write("3. **é¢„æµ‹ä¸€è‡´æ€§**: ä¸¤ç§æ¨¡å‹åœ¨ç›¸åŒæ ·æœ¬ä¸Šçš„é¢„æµ‹å·®å¼‚\n")
        f.write("4. **ç±»åˆ«ç‰¹å¼‚æ€§**: ä¸åŒç—…å®³ç±»åˆ«æ¿€æ´»çš„æ³¨æ„åŠ›æ¨¡å¼\n")
        f.write("5. **é”™è¯¯æ¡ˆä¾‹åˆ†æ**: æ¨¡å‹é¢„æµ‹é”™è¯¯æ—¶çš„æ³¨æ„åŠ›æ¨¡å¼\n\n")
        
        f.write("## ä½¿ç”¨å»ºè®®\n")
        f.write("1. é¦–å…ˆæŸ¥çœ‹å•æ ·æœ¬å¯¹æ¯”åˆ†æï¼Œç†è§£ä¸¤ç§æ³¨æ„åŠ›æœºåˆ¶çš„å·®å¼‚\n")
        f.write("2. æµè§ˆå„ç±»åˆ«çš„æ‰¹é‡åˆ†æï¼Œå‘ç°ç±»åˆ«ç‰¹å¼‚çš„æ³¨æ„åŠ›æ¨¡å¼\n")
        f.write("3. é‡ç‚¹å…³æ³¨é¢„æµ‹é”™è¯¯çš„æ ·æœ¬ï¼Œåˆ†ææ³¨æ„åŠ›æœºåˆ¶çš„å±€é™æ€§\n")
        f.write("4. ç»“åˆGrad-CAMå’Œæ³¨æ„åŠ›æƒé‡ï¼Œå…¨é¢ç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹\n")

if __name__ == "__main__":
    main() 